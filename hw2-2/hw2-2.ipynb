{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c894879-371f-424c-9252-8971e95bb037",
   "metadata": {},
   "source": [
    "## Add more statistics to analyze the an ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a398039-ec33-4769-b785-201eca19365f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2-2-1. model characteristics\n",
    "- 檢查一下 width, height, channel 是不是 onnx 的 input??? 果然不是，笑死那是 input 的名字，還好有訂正好了（應該？）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aae9d7a-ebc2-4e3f-8494-81c35c69ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total operators: 105\n",
      "Unique Operator: {'Unsqueeze', 'Gather', 'Reshape', 'Constant', 'Add', 'GlobalAveragePool', 'Clip', 'Conv', 'Gemm', 'Concat', 'Shape'}\n",
      "=====\n",
      "Operator[Conv]: 52\n",
      "Conv_0\n",
      "|- channel : 3\n",
      "|- height : 224\n",
      "|- width : 224\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [2, 2]\n",
      "Conv_2\n",
      "|- channel : 32\n",
      "|- height : 112\n",
      "|- width : 112\n",
      "|- dilations : [1, 1]\n",
      "|- group : 32\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_4\n",
      "|- channel : 32\n",
      "|- height : 112\n",
      "|- width : 112\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_5\n",
      "|- channel : 16\n",
      "|- height : 112\n",
      "|- width : 112\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_7\n",
      "|- channel : 96\n",
      "|- height : 112\n",
      "|- width : 112\n",
      "|- dilations : [1, 1]\n",
      "|- group : 96\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [2, 2]\n",
      "Conv_9\n",
      "|- channel : 96\n",
      "|- height : 56\n",
      "|- width : 56\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_10\n",
      "|- channel : 24\n",
      "|- height : 56\n",
      "|- width : 56\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_12\n",
      "|- channel : 144\n",
      "|- height : 56\n",
      "|- width : 56\n",
      "|- dilations : [1, 1]\n",
      "|- group : 144\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_14\n",
      "|- channel : 144\n",
      "|- height : 56\n",
      "|- width : 56\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_16\n",
      "|- channel : 24\n",
      "|- height : 56\n",
      "|- width : 56\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_18\n",
      "|- channel : 144\n",
      "|- height : 56\n",
      "|- width : 56\n",
      "|- dilations : [1, 1]\n",
      "|- group : 144\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [2, 2]\n",
      "Conv_20\n",
      "|- channel : 144\n",
      "|- height : 28\n",
      "|- width : 28\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_21\n",
      "|- channel : 32\n",
      "|- height : 28\n",
      "|- width : 28\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_23\n",
      "|- channel : 192\n",
      "|- height : 28\n",
      "|- width : 28\n",
      "|- dilations : [1, 1]\n",
      "|- group : 192\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_25\n",
      "|- channel : 192\n",
      "|- height : 28\n",
      "|- width : 28\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_27\n",
      "|- channel : 32\n",
      "|- height : 28\n",
      "|- width : 28\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_29\n",
      "|- channel : 192\n",
      "|- height : 28\n",
      "|- width : 28\n",
      "|- dilations : [1, 1]\n",
      "|- group : 192\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_31\n",
      "|- channel : 192\n",
      "|- height : 28\n",
      "|- width : 28\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_33\n",
      "|- channel : 32\n",
      "|- height : 28\n",
      "|- width : 28\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_35\n",
      "|- channel : 192\n",
      "|- height : 28\n",
      "|- width : 28\n",
      "|- dilations : [1, 1]\n",
      "|- group : 192\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [2, 2]\n",
      "Conv_37\n",
      "|- channel : 192\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_38\n",
      "|- channel : 64\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_40\n",
      "|- channel : 384\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 384\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_42\n",
      "|- channel : 384\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_44\n",
      "|- channel : 64\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_46\n",
      "|- channel : 384\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 384\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_48\n",
      "|- channel : 384\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_50\n",
      "|- channel : 64\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_52\n",
      "|- channel : 384\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 384\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_54\n",
      "|- channel : 384\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_56\n",
      "|- channel : 64\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_58\n",
      "|- channel : 384\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 384\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_60\n",
      "|- channel : 384\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_61\n",
      "|- channel : 96\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_63\n",
      "|- channel : 576\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 576\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_65\n",
      "|- channel : 576\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_67\n",
      "|- channel : 96\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_69\n",
      "|- channel : 576\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 576\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_71\n",
      "|- channel : 576\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_73\n",
      "|- channel : 96\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_75\n",
      "|- channel : 576\n",
      "|- height : 14\n",
      "|- width : 14\n",
      "|- dilations : [1, 1]\n",
      "|- group : 576\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [2, 2]\n",
      "Conv_77\n",
      "|- channel : 576\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_78\n",
      "|- channel : 160\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_80\n",
      "|- channel : 960\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 960\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_82\n",
      "|- channel : 960\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_84\n",
      "|- channel : 160\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_86\n",
      "|- channel : 960\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 960\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_88\n",
      "|- channel : 960\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_90\n",
      "|- channel : 160\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_92\n",
      "|- channel : 960\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 960\n",
      "|- kernel_shape : [3, 3]\n",
      "|- pads : [1, 1, 1, 1]\n",
      "|- strides : [1, 1]\n",
      "Conv_94\n",
      "|- channel : 960\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "Conv_95\n",
      "|- channel : 320\n",
      "|- height : 7\n",
      "|- width : 7\n",
      "|- dilations : [1, 1]\n",
      "|- group : 1\n",
      "|- kernel_shape : [1, 1]\n",
      "|- pads : [0, 0, 0, 0]\n",
      "|- strides : [1, 1]\n",
      "====\n",
      "Operator[Clip]: 35\n",
      "Clip_1\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_3\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_6\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_8\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_11\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_13\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_17\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_19\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_22\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_24\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_28\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_30\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_34\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_36\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_39\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_41\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_45\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_47\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_51\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_53\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_57\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_59\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_62\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_64\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_68\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_70\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_74\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_76\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_79\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_81\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_85\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_87\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_91\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_93\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "Clip_96\n",
      "|- max : 0\n",
      "|- min : 0\n",
      "====\n",
      "Operator[Add]: 10\n",
      "Add_15\n",
      "Add_26\n",
      "Add_32\n",
      "Add_43\n",
      "Add_49\n",
      "Add_55\n",
      "Add_66\n",
      "Add_72\n",
      "Add_83\n",
      "Add_89\n",
      "====\n",
      "Operator[GlobalAveragePool]: 1\n",
      "GlobalAveragePool_97\n",
      "====\n",
      "Operator[Shape]: 1\n",
      "Shape_98\n",
      "====\n",
      "Operator[Constant]: 1\n",
      "Constant_99\n",
      "|- value : 0\n",
      "====\n",
      "Operator[Gather]: 1\n",
      "Gather_100\n",
      "|- axis : 0\n",
      "====\n",
      "Operator[Unsqueeze]: 1\n",
      "Unsqueeze_101\n",
      "|- axes : [0]\n",
      "====\n",
      "Operator[Concat]: 1\n",
      "Concat_102\n",
      "|- axis : 0\n",
      "====\n",
      "Operator[Reshape]: 1\n",
      "Reshape_103\n",
      "====\n",
      "Operator[Gemm]: 1\n",
      "Gemm_104\n",
      "|- alpha : 0\n",
      "|- beta : 0\n",
      "|- transB : 1\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "import onnx \n",
    "import json\n",
    "onnx_model = onnx.load('./mobilenetv2-10.onnx')\n",
    "onnx_model = onnx.shape_inference.infer_shapes(onnx_model)\n",
    "\n",
    "## List all tensor names in the graph\n",
    "input_nlist = [k.name for k in onnx_model.graph.input]\n",
    "initializer_nlist = [k.name for k in onnx_model.graph.initializer]\n",
    "value_info_nlist = [k.name for k in onnx_model.graph.value_info]\n",
    "\n",
    "def get_size(shape):\n",
    "    dims = []\n",
    "    ndim = len(shape.dim)\n",
    "    size = 1;\n",
    "    for i in range(ndim):\n",
    "        size = size * shape.dim[i].dim_value\n",
    "        dims.append(shape.dim[i].dim_value)\n",
    "    return dims, size\n",
    "\n",
    "##### Collect Attribute ####\n",
    "def OperatorAttr(op_type):\n",
    "    JSON_list = []\n",
    "    for i in onnx_model.graph.node:\n",
    "        if i.op_type == op_type:\n",
    "            JSON = {}\n",
    "            JSON[i.name] = {}\n",
    "            if i.op_type == 'Conv':\n",
    "                for j in i.input:\n",
    "                    if j in input_nlist:\n",
    "                        idx = input_nlist.index(j)\n",
    "                        (dims, size) = get_size(onnx_model.graph.input[idx].type.tensor_type.shape)\n",
    "                        c = dims[1]\n",
    "                        h = dims[2]\n",
    "                        w = dims[3]\n",
    "                        \n",
    "                    elif j in initializer_nlist:\n",
    "                        idx = initializer_nlist.index(j)\n",
    "                        dims = onnx_model.graph.initializer[idx].dims\n",
    "                    elif j in value_info_nlist:\n",
    "                        idx = value_info_nlist.index(j)\n",
    "                        (dims, size) = get_size(onnx_model.graph.value_info[idx].type.tensor_type.shape)\n",
    "                        c = dims[1]\n",
    "                        h = dims[2]\n",
    "                        w = dims[3]\n",
    "                    JSON[i.name]['channel'] = c\n",
    "                    JSON[i.name]['height'] = h\n",
    "                    JSON[i.name]['width'] = w\n",
    "            for attr in i.attribute:\n",
    "                JSON[i.name][attr.name] = attr.i if len(attr.ints) == 0 else attr.ints\n",
    "                # str.replace(old, new[, max])\n",
    "            JSON_list.append(JSON)\n",
    "    return JSON_list\n",
    "\n",
    "def Print_JSON(JSON):\n",
    "    key = list(JSON.keys())[0]\n",
    "    print(key)\n",
    "    for ckey in JSON[key]:\n",
    "        print(f'|- {ckey} : {JSON[key][ckey]}')\n",
    "    \n",
    "\n",
    "op_dict = {}\n",
    "op_operator = set()\n",
    "\n",
    "# Compute the each op_type\n",
    "for i in onnx_model.graph.node:\n",
    "    if i.op_type in op_dict:\n",
    "        op_dict[i.op_type] += 1\n",
    "    else:\n",
    "        op_dict[i.op_type] = 1\n",
    "    op_operator.add(i.op_type)\n",
    "\n",
    "print(f'Total operators: {sum(op_dict.values())}')\n",
    "print(f'Unique Operator: {op_operator}')\n",
    "print('=====')\n",
    "for op in op_dict:\n",
    "    print(f'Operator[{op}]: {op_dict[op]}')\n",
    "    JSON_list = OperatorAttr(op)\n",
    "    if len(JSON_list) == 0:\n",
    "        print('No attribute')\n",
    "    for item in JSON_list:\n",
    "        Print_JSON(item)\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84174762-1c4b-46af-835d-ac91e5d5e21d",
   "metadata": {},
   "source": [
    "### 2-2-2. Data bandwidth requirement\n",
    "- 為什麼加上 batch = 1 答案就會正常呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab78d73-4cdb-44fc-93c2-2c1ab6eb7d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/tensorflow/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /home/guofangyu/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 56.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, datasets, transforms as T\n",
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "import torch\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "x = torch.randn(1, 3, image_height, image_width, requires_grad=True)\n",
    "torch_out = mobilenet_v2(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(mobilenet_v2,              # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"mobilenet_v2_test.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output']) # the model's output names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0edcecfc-b3cc-4d73-ad10-adb6f3457c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape inference complete ...\n",
      "start\n",
      "layer                                              read_bw    write_bw    total_bw\n",
      "-----------------------------------------------  ---------  ----------  ----------\n",
      "/classifier/classifier.1/Gemm                      5129120        4000     5133120\n",
      "/features/features.2/conv/conv.1/conv.1.0/Conv     4820736     1204224     6024960\n",
      "/features/features.2/conv/conv.0/conv.0.2/Clip     4816896     4816896     9633792\n",
      "/features/features.3/conv/conv.2/Conv              1820256      301056     2121312\n",
      "/features/features.3/conv/conv.1/conv.1.0/Conv     1812096     1806336     3618432\n",
      "/features/features.4/conv/conv.1/conv.1.0/Conv     1812096      451584     2263680\n",
      "/features/features.3/conv/conv.0/conv.0.2/Clip     1806336     1806336     3612672\n",
      "/features/features.3/conv/conv.1/conv.1.2/Clip     1806336     1806336     3612672\n",
      "/features/features.4/conv/conv.0/conv.0.2/Clip     1806336     1806336     3612672\n",
      "/features/features.18/features.18.0/Conv           1706240      250880     1957120\n",
      "/features/features.1/conv/conv.1/Conv              1607744      802816     2410560\n",
      "/features/features.1/conv/conv.0/conv.0.0/Conv     1606912     1605632     3212544\n",
      "/features/features.0/features.0.2/Clip             1605632     1605632     3211264\n",
      "/features/features.1/conv/conv.0/conv.0.2/Clip     1605632     1605632     3211264\n",
      "/features/features.17/conv/conv.2/Conv             1418240       62720     1480960\n",
      "/features/features.2/conv/conv.2/Conv              1213536      301056     1514592\n",
      "/features/features.2/conv/conv.1/conv.1.2/Clip     1204224     1204224     2408448\n",
      "/features/features.2/conv/conv.0/conv.0.0/Conv      809344     4816896     5626240\n",
      "/features/features.15/conv/conv.2/Conv              803200       31360      834560\n",
      "/features/features.16/conv/conv.2/Conv              803200       31360      834560\n",
      "/features/features.12/conv/conv.2/Conv              673152       75264      748416\n",
      "/features/features.13/conv/conv.2/Conv              673152       75264      748416\n",
      "/features/features.15/conv/conv.0/conv.0.0/Conv     649600      188160      837760\n",
      "/features/features.16/conv/conv.0/conv.0.0/Conv     649600      188160      837760\n",
      "/features/features.17/conv/conv.0/conv.0.0/Conv     649600      188160      837760\n",
      "/features/features.5/conv/conv.2/Conv               626816      100352      727168\n",
      "/features/features.6/conv/conv.2/Conv               626816      100352      727168\n",
      "/features/features.5/conv/conv.1/conv.1.0/Conv      609792      602112     1211904\n",
      "/features/features.6/conv/conv.1/conv.1.0/Conv      609792      602112     1211904\n",
      "/features/features.7/conv/conv.1/conv.1.0/Conv      609792      150528      760320\n",
      "/features/features.0/features.0.0/Conv              605696     1605632     2211328\n",
      "/features/features.3/Add                            602112      301056      903168\n",
      "/features/features.5/conv/conv.0/conv.0.2/Clip      602112      602112     1204224\n",
      "/features/features.5/conv/conv.1/conv.1.2/Clip      602112      602112     1204224\n",
      "/features/features.6/conv/conv.0/conv.0.2/Clip      602112      602112     1204224\n",
      "/features/features.6/conv/conv.1/conv.1.2/Clip      602112      602112     1204224\n",
      "/features/features.7/conv/conv.0/conv.0.2/Clip      602112      602112     1204224\n",
      "/features/features.14/conv/conv.2/Conv              482176       31360      513536\n",
      "/features/features.12/conv/conv.1/conv.1.0/Conv     474624      451584      926208\n",
      "/features/features.13/conv/conv.1/conv.1.0/Conv     474624      451584      926208\n",
      "/features/features.14/conv/conv.1/conv.1.0/Conv     474624      112896      587520\n",
      "/features/features.4/conv/conv.2/Conv               470144      100352      570496\n",
      "/features/features.4/conv/conv.1/conv.1.2/Clip      451584      451584      903168\n",
      "/features/features.12/conv/conv.0/conv.0.2/Clip     451584      451584      903168\n",
      "/features/features.12/conv/conv.1/conv.1.2/Clip     451584      451584      903168\n",
      "/features/features.13/conv/conv.0/conv.0.2/Clip     451584      451584      903168\n",
      "/features/features.13/conv/conv.1/conv.1.2/Clip     451584      451584      903168\n",
      "/features/features.14/conv/conv.0/conv.0.2/Clip     451584      451584      903168\n",
      "/features/features.11/conv/conv.2/Conv              448896       75264      524160\n",
      "/features/features.8/conv/conv.2/Conv               399616       50176      449792\n",
      "/features/features.9/conv/conv.2/Conv               399616       50176      449792\n",
      "/features/features.10/conv/conv.2/Conv              399616       50176      449792\n",
      "/features/features.8/conv/conv.1/conv.1.0/Conv      316416      301056      617472\n",
      "/features/features.9/conv/conv.1/conv.1.0/Conv      316416      301056      617472\n",
      "/features/features.10/conv/conv.1/conv.1.0/Conv     316416      301056      617472\n",
      "/features/features.11/conv/conv.1/conv.1.0/Conv     316416      301056      617472\n",
      "/features/features.3/conv/conv.0/conv.0.0/Conv      315456     1806336     2121792\n",
      "/features/features.4/conv/conv.0/conv.0.0/Conv      315456     1806336     2121792\n",
      "/features/features.8/conv/conv.0/conv.0.2/Clip      301056      301056      602112\n",
      "/features/features.8/conv/conv.1/conv.1.2/Clip      301056      301056      602112\n",
      "/features/features.9/conv/conv.0/conv.0.2/Clip      301056      301056      602112\n",
      "/features/features.9/conv/conv.1/conv.1.2/Clip      301056      301056      602112\n",
      "/features/features.10/conv/conv.0/conv.0.2/Clip     301056      301056      602112\n",
      "/features/features.10/conv/conv.1/conv.1.2/Clip     301056      301056      602112\n",
      "/features/features.11/conv/conv.0/conv.0.2/Clip     301056      301056      602112\n",
      "/features/features.11/conv/conv.1/conv.1.2/Clip     301056      301056      602112\n",
      "/features/features.12/conv/conv.0/conv.0.0/Conv     298752      451584      750336\n",
      "/features/features.13/conv/conv.0/conv.0.0/Conv     298752      451584      750336\n",
      "/features/features.14/conv/conv.0/conv.0.0/Conv     298752      451584      750336\n",
      "/features/features.18/features.18.2/Clip            250880      250880      501760\n",
      "/GlobalAveragePool                                  250880        5120      256000\n",
      "/features/features.15/conv/conv.1/conv.1.0/Conv     226560      188160      414720\n",
      "/features/features.16/conv/conv.1/conv.1.0/Conv     226560      188160      414720\n",
      "/features/features.17/conv/conv.1/conv.1.0/Conv     226560      188160      414720\n",
      "/features/features.5/Add                            200704      100352      301056\n",
      "/features/features.6/Add                            200704      100352      301056\n",
      "/features/features.7/conv/conv.2/Conv               199936       50176      250112\n",
      "/features/features.15/conv/conv.0/conv.0.2/Clip     188160      188160      376320\n",
      "/features/features.15/conv/conv.1/conv.1.2/Clip     188160      188160      376320\n",
      "/features/features.16/conv/conv.0/conv.0.2/Clip     188160      188160      376320\n",
      "/features/features.16/conv/conv.1/conv.1.2/Clip     188160      188160      376320\n",
      "/features/features.17/conv/conv.0/conv.0.2/Clip     188160      188160      376320\n",
      "/features/features.17/conv/conv.1/conv.1.2/Clip     188160      188160      376320\n",
      "/features/features.7/conv/conv.1/conv.1.2/Clip      150528      150528      301056\n",
      "/features/features.12/Add                           150528       75264      225792\n",
      "/features/features.13/Add                           150528       75264      225792\n",
      "/features/features.8/conv/conv.0/conv.0.0/Conv      150016      301056      451072\n",
      "/features/features.9/conv/conv.0/conv.0.0/Conv      150016      301056      451072\n",
      "/features/features.10/conv/conv.0/conv.0.0/Conv     150016      301056      451072\n",
      "/features/features.11/conv/conv.0/conv.0.0/Conv     150016      301056      451072\n",
      "/features/features.5/conv/conv.0/conv.0.0/Conv      125696      602112      727808\n",
      "/features/features.6/conv/conv.0/conv.0.0/Conv      125696      602112      727808\n",
      "/features/features.7/conv/conv.0/conv.0.0/Conv      125696      602112      727808\n",
      "/features/features.14/conv/conv.1/conv.1.2/Clip     112896      112896      225792\n",
      "/features/features.8/Add                            100352       50176      150528\n",
      "/features/features.9/Add                            100352       50176      150528\n",
      "/features/features.10/Add                           100352       50176      150528\n",
      "/features/features.15/Add                            62720       31360       94080\n",
      "/features/features.16/Add                            62720       31360       94080\n",
      "/Flatten                                              5120        5120       10240\n",
      "====================================================================================\n",
      "\n",
      "The memory bandwidth for processor to execute a whole model without on-chip-buffer is: \n",
      " 119445696 (bytes)\n",
      " 119.445696 (MB)\n",
      "\n",
      "op_name    unfound_tensor    op_type\n",
      "---------  ----------------  ---------\n",
      "====================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import shape_inference\n",
    "from os import path\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "from onnx import onnx_ml_pb2 as xpb2\n",
    "\n",
    "\n",
    "onnx_model = onnx.load(\"./mobilenet_v2_test.onnx\", load_external_data=False)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "inferred_model = shape_inference.infer_shapes(onnx_model)\n",
    "print('shape inference complete ...')\n",
    "\n",
    "def _parse_element(elem: xpb2.ValueInfoProto):\n",
    "    name = getattr(elem, 'name', \"None\")\n",
    "    data_type = \"NA\"\n",
    "    shape_str = \"NA\"\n",
    "    etype = getattr(elem, 'type', False)\n",
    "    if etype:\n",
    "        ttype = getattr(etype, 'tensor_type', False)\n",
    "        if ttype:\n",
    "            data_type = getattr(ttype, 'elem_type', 0)\n",
    "            shape = getattr(elem.type.tensor_type, \"shape\", False)\n",
    "            if shape:\n",
    "                shape_str = \"[\"\n",
    "                dims = getattr(shape, 'dim', [])\n",
    "                for dim in dims:\n",
    "                    vals = getattr(dim, 'dim_value', \"?\")\n",
    "                    shape_str += (str(vals) + \",\")\n",
    "                shape_str = shape_str.rstrip(\",\")\n",
    "                shape_str += \"]\"\n",
    "    return name, data_type, shape_str\n",
    "\n",
    "def get_valueproto_or_tensorproto_by_name(name: str, graph: xpb2.GraphProto):\n",
    "    for i, node in enumerate(inferred_model.graph.node):\n",
    "            if node.name == \"\":\n",
    "                inferred_model.graph.node[i].name = str(i)\n",
    "    input_nlist = [k.name for k in graph.input]\n",
    "    initializer_nlist = [k.name for k in graph.initializer]\n",
    "    value_info_nlist = [k.name for k in graph.value_info]\n",
    "    output_nlist = [k.name for k in graph.output]\n",
    "\n",
    "    # get tensor data\n",
    "    if name in input_nlist:\n",
    "        idx = input_nlist.index(name)\n",
    "        return graph.input[idx], int(1)\n",
    "    elif name in value_info_nlist:\n",
    "        idx = value_info_nlist.index(name)\n",
    "        return graph.value_info[idx], int(2)\n",
    "    elif name in initializer_nlist:\n",
    "        idx = initializer_nlist.index(name)\n",
    "        return graph.initializer[idx], int(3)\n",
    "    elif name in output_nlist:\n",
    "        idx = output_nlist.index(name)\n",
    "        return graph.output[idx], int(4)\n",
    "    else:\n",
    "        print(\"[ERROR MASSAGE] Can't find the tensor: \", name)\n",
    "        print('input_nlist:\\n', input_nlist)\n",
    "        print('===================')\n",
    "        print('value_info_nlist:\\n', value_info_nlist)\n",
    "        print('===================')\n",
    "        print('initializer_nlist:\\n', initializer_nlist)\n",
    "        print('===================')\n",
    "        print('output_nlist:\\n', output_nlist)\n",
    "        print('===================')\n",
    "        return False, 0\n",
    "\n",
    "def cal_tensor_mem_size(elem_type: str, shape: [int]):\n",
    "    \"\"\" given the element type of the tensor and its shape, and return its memory size.\n",
    "\n",
    "    Utility.\n",
    "\n",
    "    Args:\n",
    "        ttype: the type of the element of the given tensor. format: 'int', ...\n",
    "        shape: the shape of the given tensor. format: [] of int\n",
    "\n",
    "    Returns:\n",
    "        mem_size: int\n",
    "    \"\"\"\n",
    "    # init\n",
    "    mem_size = int(1)\n",
    "    # traverse the list to get the number of the elements\n",
    "    # print(shape)\n",
    "    for num in shape:\n",
    "        mem_size *= num\n",
    "    # multiple the size of variable with the number of the elements\n",
    "    # \"FLOAT\": 1,\n",
    "    # \"UINT8\": 2,\n",
    "    # \"INT8\": 3,\n",
    "    # \"UINT16\": 4,\n",
    "    # \"INT16\": 5,\n",
    "    # \"INT32\": 6,\n",
    "    # \"INT64\": 7,\n",
    "    # # \"STRING\" : 8,\n",
    "    # \"BOOL\": 9,\n",
    "    # \"FLOAT16\": 10,\n",
    "    # \"DOUBLE\": 11,\n",
    "    # \"UINT32\": 12,\n",
    "    # \"UINT64\": 13,\n",
    "    # \"COMPLEX64\": 14,\n",
    "    # \"COMPLEX128\": 15\n",
    "    if elem_type == 1:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 2:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 3:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 4:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 5:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 6:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 7:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 9:\n",
    "        mem_size *= 1\n",
    "    elif elem_type == 10:\n",
    "        mem_size *= 2\n",
    "    elif elem_type == 11:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 12:\n",
    "        mem_size *= 4\n",
    "    elif elem_type == 13:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 14:\n",
    "        mem_size *= 8\n",
    "    elif elem_type == 15:\n",
    "        mem_size *= 16\n",
    "    else:\n",
    "        print(\"Undefined data type\")\n",
    "\n",
    "    return mem_size\n",
    "\n",
    "\n",
    "\n",
    "def get_bandwidth(graph: xpb2.GraphProto):\n",
    "    try:\n",
    "        mem_BW_list = []\n",
    "        total_mem_BW = 0\n",
    "        unknown_tensor_list = []\n",
    "        # traverse all the nodes\n",
    "        for nodeProto in graph.node:\n",
    "            if nodeProto.op_type == 'Constant':\n",
    "                continue\n",
    "            # init variables\n",
    "            read_mem_BW_each_layer = 0\n",
    "            write_mem_BW_each_layer = 0\n",
    "            total_each_layer = 0\n",
    "            # traverse all input tensor\n",
    "            for input_name in nodeProto.input:\n",
    "                # get the TensorProto/ValueInfoProto by searching its name\n",
    "                proto, type_Num = get_valueproto_or_tensorproto_by_name(\n",
    "                    input_name, graph)\n",
    "                # parse the ValueInfoProto/TensorProto\n",
    "                if proto:\n",
    "                    if type_Num == 3:\n",
    "                        dtype = getattr(proto, 'data_type', False)\n",
    "                        # get the shape of the tensor\n",
    "                        shape = getattr(proto, 'dims', [])\n",
    "                    elif type_Num == 1 or type_Num == 2:\n",
    "                        name, dtype, shape_str = _parse_element(proto)\n",
    "                        shape_str = shape_str.strip('[]')\n",
    "                        shape_str = shape_str.split(',')\n",
    "                        shape = []\n",
    "                        for dim in shape_str:\n",
    "                            try:\n",
    "                                shape.append(int(dim))\n",
    "                            except:\n",
    "                                shape.append(0)\n",
    "                    else:\n",
    "                        print(\n",
    "                            '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                            input_name, ' is from a wrong list !')\n",
    "                else:\n",
    "                    print(\n",
    "                        '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                        input_name, ' is no found !')\n",
    "                    unknown_tensor_list.append(\n",
    "                        (nodeProto.name, input_name, nodeProto.op_type))\n",
    "                # calculate the tensor size in btye\n",
    "                \n",
    "                read_mem_BW_each_layer += cal_tensor_mem_size(dtype, shape)\n",
    "\n",
    "            # traverse all output tensor\n",
    "            for output_name in nodeProto.output:\n",
    "                # get the TensorProto/ValueInfoProto by searching its name\n",
    "                proto, type_Num = get_valueproto_or_tensorproto_by_name(\n",
    "                    output_name, graph)\n",
    "                # parse the ValueInfoProto\n",
    "                if proto:\n",
    "                    if type_Num == 2 or type_Num == 4:\n",
    "                        # name, dtype, shape = utils._parse_ValueInfoProto(proto)\n",
    "                        name, dtype, shape_str = _parse_element(proto)\n",
    "                        shape_str = shape_str.strip('[]')\n",
    "                        shape_str = shape_str.split(',')\n",
    "                        shape = []\n",
    "                        for dim in shape_str:\n",
    "                            try:\n",
    "                                shape.append(int(dim))\n",
    "                            except:\n",
    "                                shape.append(0)\n",
    "                            \n",
    "                    else:\n",
    "                        print(\n",
    "                            '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                            output_name, ' is from a wrong list !')\n",
    "                else:\n",
    "                    print(\n",
    "                        '[ERROR MASSAGE] [get_info/mem_BW_without_buf] The Tensor: ',\n",
    "                        input_name, ' is no found !')\n",
    "                    unknown_tensor_list.append(\n",
    "                        (nodeProto.name, output_name, nodeProto.op_type))\n",
    "                # calculate the tensor size in btye\n",
    "                write_mem_BW_each_layer += cal_tensor_mem_size(dtype, shape)\n",
    "            # cal total bw\n",
    "            total_each_layer = read_mem_BW_each_layer + write_mem_BW_each_layer\n",
    "\n",
    "            # store into tuple\n",
    "            temp_tuple = (nodeProto.name, read_mem_BW_each_layer,\n",
    "                        write_mem_BW_each_layer, total_each_layer)\n",
    "            #append it\n",
    "            mem_BW_list.append(temp_tuple)\n",
    "            # accmulate the value\n",
    "            total_mem_BW += total_each_layer\n",
    "\n",
    "        # display the mem_bw of eahc layer\n",
    "        columns = ['layer', 'read_bw', 'write_bw', 'total_bw']\n",
    "        # resort the list\n",
    "        mem_BW_list = sorted(mem_BW_list,\n",
    "                             key=lambda Layer: Layer[1],\n",
    "                             reverse=True)\n",
    "        print(tabulate(mem_BW_list, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "        # display it\n",
    "        print(\n",
    "            \"The memory bandwidth for processor to execute a whole model without on-chip-buffer is: \\n\",\n",
    "            total_mem_BW, '(bytes)\\n',\n",
    "            float(total_mem_BW) / float(1000000), '(MB)\\n')\n",
    "        # display the unknown tensor\n",
    "        columns = ['op_name', 'unfound_tensor', 'op_type']\n",
    "        print(tabulate(unknown_tensor_list, headers=columns))\n",
    "        print(\n",
    "            '====================================================================================\\n'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR MASSAGE] Unable to display: \" + str(e))\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "#從這裡開始\n",
    "print(\"start\")\n",
    "get_bandwidth(inferred_model.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef46c8dc-8a96-4576-aa66-e30212264d8b",
   "metadata": {},
   "source": [
    "### 2-2-3. activation memory storage requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad8a6937-5162-44bd-8904-feefd198759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation memory storage requirement: 107117792 byte (102.16MB)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "activation = {}\n",
    "# Define a hook function\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Load a pre-trained AlexNet model\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Dictionary to store activations from each layer\n",
    "activation = {}\n",
    "\n",
    "# Register hook to each linear layer\n",
    "for layer_name, layer in model.named_modules():\n",
    "    layer.register_forward_hook(get_activation(layer_name))\n",
    "\n",
    "# Run model inference\n",
    "data = torch.randn(1, 3, 224, 224)\n",
    "output = model(data)\n",
    "\n",
    "# Access the saved activations\n",
    "local_memory = 0\n",
    "for layer in activation:\n",
    "    # 所有 layer 的 tensor 皆為 float32，因此以 4 byte 計算\n",
    "    local_memory += torch.numel(activation[layer])*4\n",
    "    # print(f\"Activation from layer {layer}: {activation[layer].shape}\")\n",
    "\n",
    "print(f\"Activation memory storage requirement: {local_memory} byte ({round(local_memory/1048576, 2)}MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ca819e-7d3e-495a-bb68-ba02074f0a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401408"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e42844-3dde-4efc-b94c-6a91b74602b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
